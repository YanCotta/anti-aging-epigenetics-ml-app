{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895ee7de",
   "metadata": {},
   "source": [
    "# Anti-Aging ML Experiment 02: Random Forest with ONNX Export and SHAP Explanations\n",
    "\n",
    "**Experiment Date:** October 16, 2025  \n",
    "**Issue:** #6 - Random Forest baseline with ONNX + SHAP  \n",
    "**Previous Baseline:** Linear Regression (R¬≤=0.978, MAE=2.01)  \n",
    "\n",
    "## Experiment Overview\n",
    "\n",
    "This notebook implements a comprehensive Random Forest pipeline for biological age prediction, addressing **Issue #6** from our development plan:\n",
    "\n",
    "### Objectives\n",
    "1. **Model Training**: Train Random Forest with RandomizedSearchCV hyperparameter optimization\n",
    "2. **ONNX Export**: Convert trained model to ONNX format for production inference\n",
    "3. **SHAP Explanations**: Implement TreeExplainer for feature importance and prediction interpretability\n",
    "4. **MLflow Tracking**: Log all metrics, parameters, and artifacts for comparison with Linear baseline\n",
    "5. **Publication Figures**: Generate visualizations for thesis documentation\n",
    "\n",
    "### Research Questions\n",
    "- Does Random Forest outperform Linear Regression for biological age prediction?\n",
    "- Which features are most important according to ensemble methods?\n",
    "- How do SHAP values differ from traditional feature importance?\n",
    "- Is the performance improvement (if any) statistically significant?\n",
    "\n",
    "### Key Methodology Points\n",
    "- **Data Leakage Prevention**: Preprocessing fitted only on training data\n",
    "- **Hyperparameter Tuning**: RandomizedSearchCV with 100 iterations\n",
    "- **Cross-Validation**: 5-fold CV with proper stratification\n",
    "- **Statistical Rigor**: Bootstrap confidence intervals for metrics\n",
    "- **Reproducibility**: Fixed random seeds throughout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb05cf1d",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cc2c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up project paths and environment\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add backend API to Python path for imports\n",
    "project_root = Path.cwd().parent\n",
    "backend_path = project_root / \"antiaging-mvp\" / \"backend\"\n",
    "sys.path.insert(0, str(backend_path))\n",
    "\n",
    "print(f\"üìÇ Project root: {project_root}\")\n",
    "print(f\"üìÇ Backend path: {backend_path}\")\n",
    "print(f\"üìÇ Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Create directories for outputs\n",
    "reports_dir = Path.cwd() / \"reports\" / \"02_random_forest\"\n",
    "figures_dir = reports_dir / \"figures\"\n",
    "models_dir = reports_dir / \"models\"\n",
    "\n",
    "for d in [reports_dir, figures_dir, models_dir]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úì Created: {d}\")\n",
    "\n",
    "# Verify data files exist\n",
    "data_path = backend_path / \"api\" / \"data\" / \"datasets\" / \"train.csv\"\n",
    "print(f\"\\nüìä Training data exists: {data_path.exists()}\")\n",
    "print(f\"üìä Data path: {data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6872738c",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for Random Forest training, ONNX conversion, SHAP explanations, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a904dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn for ML\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# ONNX conversion\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnxruntime as rt\n",
    "\n",
    "# SHAP for explanations\n",
    "import shap\n",
    "\n",
    "# MLflow for experiment tracking\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Project imports\n",
    "from api.ml.preprocessor import DataPreprocessor\n",
    "import joblib\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úì All libraries imported successfully\")\n",
    "print(f\"‚úì Random seed set to: {RANDOM_STATE}\")\n",
    "print(f\"‚úì Experiment timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee54e46",
   "metadata": {},
   "source": [
    "## 3. Load and Explore Training Data\n",
    "\n",
    "Load the biologically realistic training dataset (6,000 samples) with proper age correlation (0.657)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea59ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data\n",
    "TARGET_COL = \"biological_age\"\n",
    "DROP_FEATURES = [\"user_id\"]  # Non-predictive identifiers\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(f\"‚úì Data loaded successfully\")\n",
    "print(f\"üìä Dataset shape: {df.shape}\")\n",
    "print(f\"üìä Features: {df.shape[1]} columns\")\n",
    "print(f\"üìä Samples: {df.shape[0]:,} rows\")\n",
    "\n",
    "# Basic data quality checks\n",
    "print(f\"\\nüîç Data Quality:\")\n",
    "print(f\"  - Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"  - Duplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"  - Target range: [{df[TARGET_COL].min():.1f}, {df[TARGET_COL].max():.1f}]\")\n",
    "print(f\"  - Age correlation with target: {df['age'].corr(df[TARGET_COL]):.3f}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nüìã Dataset preview:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db34cd89",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split and Preprocessing\n",
    "\n",
    "**Critical**: Split data FIRST, then fit preprocessor only on training data to prevent data leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7108d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop(columns=[TARGET_COL] + [c for c in DROP_FEATURES if c in df.columns])\n",
    "y = df[TARGET_COL]\n",
    "\n",
    "print(f\"‚úì Features shape: {X.shape}\")\n",
    "print(f\"‚úì Target shape: {y.shape}\")\n",
    "\n",
    "# Split data (80/20 split)\n",
    "TEST_SIZE = 0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"\\nüìä Split Summary:\")\n",
    "print(f\"  - Training samples: {X_train.shape[0]:,} ({100*(1-TEST_SIZE):.0f}%)\")\n",
    "print(f\"  - Test samples: {X_test.shape[0]:,} ({100*TEST_SIZE:.0f}%)\")\n",
    "print(f\"  - Training target range: [{y_train.min():.1f}, {y_train.max():.1f}]\")\n",
    "print(f\"  - Test target range: [{y_test.min():.1f}, {y_test.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1ff15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing (fit ONLY on training data)\n",
    "preprocessor = DataPreprocessor()\n",
    "\n",
    "print(\"üîÑ Fitting preprocessor on training data...\")\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "print(f\"‚úì Training data preprocessed: {X_train_processed.shape}\")\n",
    "\n",
    "print(\"üîÑ Transforming test data...\")\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "print(f\"‚úì Test data preprocessed: {X_test_processed.shape}\")\n",
    "\n",
    "# Save preprocessor for later use\n",
    "preprocessor_path = models_dir / \"preprocessor_rf.pkl\"\n",
    "preprocessor.save(str(preprocessor_path))\n",
    "print(f\"‚úì Preprocessor saved to: {preprocessor_path}\")\n",
    "\n",
    "# Store feature names for SHAP analysis\n",
    "feature_names = list(X.columns)\n",
    "print(f\"\\nüìã Total features after preprocessing: {X_train_processed.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cf27f5",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter Tuning with RandomizedSearchCV\n",
    "\n",
    "Using RandomizedSearchCV (100 iterations) to find optimal Random Forest hyperparameters. This balances thoroughness with computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a470ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter search space\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 500),           # Number of trees\n",
    "    'max_depth': randint(10, 50),                # Maximum tree depth\n",
    "    'min_samples_split': randint(2, 20),         # Minimum samples to split\n",
    "    'min_samples_leaf': randint(1, 10),          # Minimum samples per leaf\n",
    "    'max_features': uniform(0.3, 0.7),           # Features per split (30-100%)\n",
    "    'bootstrap': [True],                          # Use bootstrap sampling\n",
    "    'random_state': [RANDOM_STATE]                # Fixed for reproducibility\n",
    "}\n",
    "\n",
    "print(\"üîç Hyperparameter Search Space:\")\n",
    "for param, values in param_distributions.items():\n",
    "    print(f\"  - {param}: {values}\")\n",
    "\n",
    "# Initialize base Random Forest\n",
    "rf_base = RandomForestRegressor(n_jobs=-1, random_state=RANDOM_STATE)\n",
    "\n",
    "# Configure RandomizedSearchCV\n",
    "N_ITER = 100  # Number of random combinations to try\n",
    "CV_FOLDS = 5   # Cross-validation folds\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf_base,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=N_ITER,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='neg_mean_absolute_error',  # Optimize for MAE\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  RandomizedSearchCV Configuration:\")\n",
    "print(f\"  - Iterations: {N_ITER}\")\n",
    "print(f\"  - CV Folds: {CV_FOLDS}\")\n",
    "print(f\"  - Scoring: negative MAE\")\n",
    "print(f\"  - Total fits: {N_ITER * CV_FOLDS:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee5c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run hyperparameter search\n",
    "print(\"üöÄ Starting hyperparameter search...\")\n",
    "print(f\"‚è±Ô∏è  This will take several minutes (training {N_ITER * CV_FOLDS:,} models)...\\n\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "random_search.fit(X_train_processed, y_train)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"\\n‚úì Hyperparameter search completed in {elapsed_time:.1f} seconds ({elapsed_time/60:.1f} minutes)\")\n",
    "\n",
    "# Display best parameters\n",
    "print(f\"\\nüèÜ Best Parameters Found:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  - {param}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  - {param}: {value}\")\n",
    "\n",
    "print(f\"\\nüìä Best Cross-Validation Score: {-random_search.best_score_:.4f} (MAE)\")\n",
    "\n",
    "# Get the best model\n",
    "best_rf = random_search.best_estimator_\n",
    "print(f\"\\n‚úì Best model retrieved and ready for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadaf2aa",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation on Test Set\n",
    "\n",
    "Evaluate the best Random Forest model on the hold-out test set and compare with Linear Regression baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66848bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on test set\n",
    "y_pred_train = best_rf.predict(X_train_processed)\n",
    "y_pred_test = best_rf.predict(X_test_processed)\n",
    "\n",
    "# Calculate metrics for training set\n",
    "train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "train_r2 = r2_score(y_train, y_pred_train)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "test_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "# Display results\n",
    "print(\"=\"*60)\n",
    "print(\"RANDOM FOREST PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìä Training Set Performance:\")\n",
    "print(f\"  - R¬≤ Score: {train_r2:.4f}\")\n",
    "print(f\"  - RMSE: {train_rmse:.4f} years\")\n",
    "print(f\"  - MAE: {train_mae:.4f} years\")\n",
    "\n",
    "print(\"\\nüìä Test Set Performance:\")\n",
    "print(f\"  - R¬≤ Score: {test_r2:.4f}\")\n",
    "print(f\"  - RMSE: {test_rmse:.4f} years\")\n",
    "print(f\"  - MAE: {test_mae:.4f} years\")\n",
    "\n",
    "# Calculate overfitting gap\n",
    "overfit_gap = train_r2 - test_r2\n",
    "print(f\"\\n‚ö†Ô∏è  Overfitting Gap (Train R¬≤ - Test R¬≤): {overfit_gap:.4f}\")\n",
    "\n",
    "# Compare with Linear Regression baseline\n",
    "lr_test_r2 = 0.9691  # From Issue #21\n",
    "lr_test_mae = 2.05\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON WITH LINEAR REGRESSION BASELINE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nLinear Regression (Baseline):\")\n",
    "print(f\"  - R¬≤ Score: {lr_test_r2:.4f}\")\n",
    "print(f\"  - MAE: {lr_test_mae:.4f} years\")\n",
    "\n",
    "print(f\"\\nRandom Forest (Current):\")\n",
    "print(f\"  - R¬≤ Score: {test_r2:.4f}\")\n",
    "print(f\"  - MAE: {test_mae:.4f} years\")\n",
    "\n",
    "print(f\"\\nImprovement:\")\n",
    "r2_improvement = test_r2 - lr_test_r2\n",
    "mae_improvement = lr_test_mae - test_mae\n",
    "print(f\"  - Œî R¬≤: {r2_improvement:+.4f} ({100*r2_improvement/lr_test_r2:+.2f}%)\")\n",
    "print(f\"  - Œî MAE: {mae_improvement:+.4f} years ({100*mae_improvement/lr_test_mae:+.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52451773",
   "metadata": {},
   "source": [
    "## 7. Visualization: Prediction vs Actual\n",
    "\n",
    "Publication-quality scatter plot showing predicted vs actual biological age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e18a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction scatter plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Training set\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_train, y_pred_train, alpha=0.3, s=20, label='Training samples')\n",
    "ax1.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], \n",
    "         'r--', lw=2, label='Perfect prediction')\n",
    "ax1.set_xlabel('Actual Biological Age (years)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Predicted Biological Age (years)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title(f'Training Set: R¬≤={train_r2:.4f}, MAE={train_mae:.2f} years', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.legend(loc='upper left', fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Test set\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(y_test, y_pred_test, alpha=0.5, s=30, color='green', label='Test samples')\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "         'r--', lw=2, label='Perfect prediction')\n",
    "ax2.set_xlabel('Actual Biological Age (years)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Predicted Biological Age (years)', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Test Set: R¬≤={test_r2:.4f}, MAE={test_mae:.2f} years', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.legend(loc='upper left', fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'rf_predictions_scatter.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Figure saved: {figures_dir / 'rf_predictions_scatter.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeae8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual analysis\n",
    "residuals_test = y_test - y_pred_test\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Residuals vs Predicted\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_pred_test, residuals_test, alpha=0.5, s=30, color='purple')\n",
    "ax1.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "ax1.set_xlabel('Predicted Biological Age (years)', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Residuals (Actual - Predicted)', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Residual Plot: Random Forest Test Set', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals distribution\n",
    "ax2 = axes[1]\n",
    "ax2.hist(residuals_test, bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(x=0, color='r', linestyle='--', lw=2)\n",
    "ax2.set_xlabel('Residuals (years)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax2.set_title(f'Residual Distribution (Œº={residuals_test.mean():.2f}, œÉ={residuals_test.std():.2f})', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'rf_residuals_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Figure saved: {figures_dir / 'rf_residuals_analysis.png'}\")\n",
    "print(f\"\\nüìä Residual Statistics:\")\n",
    "print(f\"  - Mean: {residuals_test.mean():.4f} years\")\n",
    "print(f\"  - Std Dev: {residuals_test.std():.4f} years\")\n",
    "print(f\"  - Min: {residuals_test.min():.4f} years\")\n",
    "print(f\"  - Max: {residuals_test.max():.4f} years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c8970f",
   "metadata": {},
   "source": [
    "## 8. ONNX Model Export\n",
    "\n",
    "Convert the trained Random Forest to ONNX format for production deployment and cross-platform inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19319994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define initial types for ONNX conversion\n",
    "n_features = X_train_processed.shape[1]\n",
    "initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
    "\n",
    "print(f\"üîÑ Converting Random Forest to ONNX format...\")\n",
    "print(f\"  - Input features: {n_features}\")\n",
    "print(f\"  - Model trees: {best_rf.n_estimators}\")\n",
    "\n",
    "# Convert to ONNX\n",
    "onnx_model = convert_sklearn(\n",
    "    best_rf,\n",
    "    initial_types=initial_type,\n",
    "    target_opset=12\n",
    ")\n",
    "\n",
    "# Save ONNX model\n",
    "onnx_path = models_dir / \"random_forest_model.onnx\"\n",
    "with open(onnx_path, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"‚úì ONNX model saved to: {onnx_path}\")\n",
    "print(f\"  - File size: {onnx_path.stat().st_size / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044cc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate ONNX model predictions match scikit-learn\n",
    "print(\"üîç Validating ONNX model predictions...\")\n",
    "\n",
    "# Load ONNX model\n",
    "sess = rt.InferenceSession(str(onnx_path))\n",
    "input_name = sess.get_inputs()[0].name\n",
    "output_name = sess.get_outputs()[0].name\n",
    "\n",
    "# Make predictions with ONNX\n",
    "X_test_onnx = X_test_processed.astype(np.float32)\n",
    "onnx_pred = sess.run([output_name], {input_name: X_test_onnx})[0].flatten()\n",
    "\n",
    "# Compare predictions\n",
    "sklearn_pred = best_rf.predict(X_test_processed)\n",
    "prediction_diff = np.abs(sklearn_pred - onnx_pred)\n",
    "\n",
    "print(f\"\\n‚úì ONNX validation complete:\")\n",
    "print(f\"  - Max prediction difference: {prediction_diff.max():.8f} years\")\n",
    "print(f\"  - Mean prediction difference: {prediction_diff.mean():.8f} years\")\n",
    "print(f\"  - Predictions match: {np.allclose(sklearn_pred, onnx_pred, rtol=1e-3)}\")\n",
    "\n",
    "# Calculate ONNX metrics\n",
    "onnx_mae = mean_absolute_error(y_test, onnx_pred)\n",
    "onnx_r2 = r2_score(y_test, onnx_pred)\n",
    "\n",
    "print(f\"\\nüìä ONNX Model Performance:\")\n",
    "print(f\"  - R¬≤ Score: {onnx_r2:.4f}\")\n",
    "print(f\"  - MAE: {onnx_mae:.4f} years\")\n",
    "print(f\"  - Difference from sklearn: {abs(test_mae - onnx_mae):.8f} years\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85938a2c",
   "metadata": {},
   "source": [
    "## 9. SHAP Feature Importance Analysis\n",
    "\n",
    "Using SHAP (SHapley Additive exPlanations) TreeExplainer for model interpretability. SHAP provides both global feature importance and local prediction explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5ea13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP TreeExplainer\n",
    "print(\"üîÑ Initializing SHAP TreeExplainer...\")\n",
    "explainer = shap.TreeExplainer(best_rf)\n",
    "\n",
    "# Calculate SHAP values for test set (use subset for speed)\n",
    "n_shap_samples = min(1000, len(X_test_processed))\n",
    "X_shap = X_test_processed[:n_shap_samples]\n",
    "\n",
    "print(f\"üîÑ Computing SHAP values for {n_shap_samples} test samples...\")\n",
    "shap_values = explainer.shap_values(X_shap)\n",
    "\n",
    "print(f\"‚úì SHAP values computed\")\n",
    "print(f\"  - Shape: {shap_values.shape}\")\n",
    "print(f\"  - Base value (expected prediction): {explainer.expected_value:.2f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e31a645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (Bar) - Global feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_shap, feature_names=feature_names, \n",
    "                  plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.title('SHAP Feature Importance (Top 20 Features)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('Mean |SHAP value| (impact on prediction)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'shap_feature_importance_bar.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Figure saved: {figures_dir / 'shap_feature_importance_bar.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e52e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (Beeswarm) - Feature impact distribution\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_shap, feature_names=feature_names, \n",
    "                  show=False, max_display=20)\n",
    "plt.title('SHAP Feature Impact Distribution (Top 20 Features)', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.xlabel('SHAP value (impact on biological age prediction)', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'shap_summary_beeswarm.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Figure saved: {figures_dir / 'shap_summary_beeswarm.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a058a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SHAP importance with traditional RF feature importance\n",
    "rf_importance = best_rf.feature_importances_\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Create comparison dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'RF_Importance': rf_importance,\n",
    "    'SHAP_Importance': mean_abs_shap\n",
    "})\n",
    "importance_df = importance_df.sort_values('SHAP_Importance', ascending=False).head(15)\n",
    "\n",
    "# Plot comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "x = np.arange(len(importance_df))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.barh(x - width/2, importance_df['RF_Importance'], width, \n",
    "                label='Random Forest Importance', alpha=0.8, color='steelblue')\n",
    "bars2 = ax.barh(x + width/2, importance_df['SHAP_Importance'], width, \n",
    "                label='SHAP Importance', alpha=0.8, color='coral')\n",
    "\n",
    "ax.set_ylabel('Feature', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Feature Importance Comparison: RF vs SHAP (Top 15)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(importance_df['Feature'])\n",
    "ax.legend(loc='lower right', fontsize=11)\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'feature_importance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Figure saved: {figures_dir / 'feature_importance_comparison.png'}\")\n",
    "print(f\"\\nüìä Top 5 Features by SHAP Importance:\")\n",
    "print(importance_df[['Feature', 'SHAP_Importance']].head().to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d32896b",
   "metadata": {},
   "source": [
    "## 10. MLflow Experiment Tracking\n",
    "\n",
    "Log all metrics, parameters, and artifacts to MLflow for comparison with Linear Regression baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc94840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow experiment\n",
    "EXPERIMENT_NAME = \"anti-aging-ml-comparison\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"üîÑ Logging to MLflow experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=\"random_forest_onnx_shap\") as run:\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params({\n",
    "        \"model\": \"RandomForestRegressor\",\n",
    "        \"n_estimators\": best_rf.n_estimators,\n",
    "        \"max_depth\": best_rf.max_depth,\n",
    "        \"min_samples_split\": best_rf.min_samples_split,\n",
    "        \"min_samples_leaf\": best_rf.min_samples_leaf,\n",
    "        \"max_features\": best_rf.max_features,\n",
    "        \"bootstrap\": best_rf.bootstrap,\n",
    "        \"n_features\": X_train_processed.shape[1],\n",
    "        \"test_size\": TEST_SIZE,\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"hp_search_iterations\": N_ITER,\n",
    "        \"cv_folds\": CV_FOLDS\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metrics({\n",
    "        \"train_r2\": train_r2,\n",
    "        \"train_rmse\": train_rmse,\n",
    "        \"train_mae\": train_mae,\n",
    "        \"test_r2\": test_r2,\n",
    "        \"test_rmse\": test_rmse,\n",
    "        \"test_mae\": test_mae,\n",
    "        \"cv_mae_best\": -random_search.best_score_,\n",
    "        \"overfitting_gap\": overfit_gap\n",
    "    })\n",
    "    \n",
    "    # Log comparison with Linear Regression\n",
    "    mlflow.log_metrics({\n",
    "        \"r2_improvement_vs_lr\": r2_improvement,\n",
    "        \"mae_improvement_vs_lr\": mae_improvement\n",
    "    })\n",
    "    \n",
    "    # Save and log model artifacts\n",
    "    model_pkl_path = models_dir / \"random_forest_model.pkl\"\n",
    "    joblib.dump(best_rf, model_pkl_path)\n",
    "    \n",
    "    mlflow.log_artifact(str(model_pkl_path))\n",
    "    mlflow.log_artifact(str(onnx_path))\n",
    "    mlflow.log_artifact(str(preprocessor_path))\n",
    "    \n",
    "    # Log figures\n",
    "    for fig_path in figures_dir.glob(\"*.png\"):\n",
    "        mlflow.log_artifact(str(fig_path))\n",
    "    \n",
    "    # Log model to MLflow\n",
    "    mlflow.sklearn.log_model(best_rf, \"random_forest_model\")\n",
    "    \n",
    "    run_id = run.info.run_id\n",
    "    print(f\"\\n‚úì MLflow logging complete\")\n",
    "    print(f\"  - Run ID: {run_id}\")\n",
    "    print(f\"  - Experiment: {EXPERIMENT_NAME}\")\n",
    "    print(f\"  - Artifacts logged: {len(list(figures_dir.glob('*.png'))) + 3}\")\n",
    "\n",
    "print(f\"\\nüìä View results in MLflow UI:\")\n",
    "print(f\"  cd {project_root / 'antiaging-mvp' / 'backend'}\")\n",
    "print(f\"  mlflow ui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5c48b1",
   "metadata": {},
   "source": [
    "## 11. Experiment Summary and Conclusions\n",
    "\n",
    "Final summary of Random Forest performance, key findings, and next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95412b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary\n",
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT 02: RANDOM FOREST - FINAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nüìä MODEL PERFORMANCE\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Metric':<30} {'Linear Reg':>15} {'Random Forest':>15} {'Œî':>10}\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"{'Test R¬≤ Score':<30} {lr_test_r2:>15.4f} {test_r2:>15.4f} {r2_improvement:>+10.4f}\")\n",
    "print(f\"{'Test MAE (years)':<30} {lr_test_mae:>15.4f} {test_mae:>15.4f} {mae_improvement:>+10.4f}\")\n",
    "print(f\"{'Test RMSE (years)':<30} {'N/A':>15} {test_rmse:>15.4f} {'N/A':>10}\")\n",
    "\n",
    "print(\"\\nüìà KEY FINDINGS\")\n",
    "print(\"-\" * 70)\n",
    "print(f\"1. Model Complexity:\")\n",
    "print(f\"   - Trees: {best_rf.n_estimators}\")\n",
    "print(f\"   - Max Depth: {best_rf.max_depth}\")\n",
    "print(f\"   - Features per split: {best_rf.max_features:.2f}\")\n",
    "\n",
    "print(f\"\\n2. Generalization:\")\n",
    "print(f\"   - Overfitting gap: {overfit_gap:.4f}\")\n",
    "print(f\"   - CV Best MAE: {-random_search.best_score_:.4f} years\")\n",
    "print(f\"   - Test MAE: {test_mae:.4f} years\")\n",
    "\n",
    "print(f\"\\n3. Interpretability:\")\n",
    "print(f\"   - SHAP analysis completed for {n_shap_samples} samples\")\n",
    "print(f\"   - Top feature importance: {importance_df.iloc[0]['Feature']}\")\n",
    "print(f\"   - SHAP importance: {importance_df.iloc[0]['SHAP_Importance']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. Deployment Readiness:\")\n",
    "print(f\"   - ONNX model size: {onnx_path.stat().st_size / 1024:.2f} KB\")\n",
    "print(f\"   - ONNX predictions match sklearn: {np.allclose(sklearn_pred, onnx_pred, rtol=1e-3)}\")\n",
    "print(f\"   - Preprocessor saved: ‚úì\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  CRITICAL ANALYSIS\")\n",
    "print(\"-\" * 70)\n",
    "if abs(r2_improvement) < 0.01:\n",
    "    print(\"‚ö†Ô∏è  Minimal performance difference from Linear Regression\")\n",
    "    print(\"   This suggests linear relationships dominate the data\")\n",
    "if overfit_gap > 0.05:\n",
    "    print(\"‚ö†Ô∏è  Significant overfitting detected\")\n",
    "    print(f\"   Train R¬≤={train_r2:.4f} >> Test R¬≤={test_r2:.4f}\")\n",
    "    \n",
    "print(\"\\nüéØ NEXT STEPS\")\n",
    "print(\"-\" * 70)\n",
    "print(\"1. Issue #7: Train MLP neural network for comparison\")\n",
    "print(\"2. Statistical significance testing (bootstrap, permutation)\")\n",
    "print(\"3. Age-stratified performance analysis\")\n",
    "print(\"4. Feature interaction analysis\")\n",
    "print(\"5. External validation on test subsets (young, elderly, healthy, unhealthy)\")\n",
    "\n",
    "print(\"\\n‚úì Experiment complete - all artifacts saved\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877addde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment metadata\n",
    "experiment_metadata = {\n",
    "    'experiment_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'experiment_name': 'Random Forest with ONNX + SHAP',\n",
    "    'issue': '#6',\n",
    "    'notebook': '02_random_forest_onnx_shap.ipynb',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'test_metrics': {\n",
    "        'r2': float(test_r2),\n",
    "        'mae': float(test_mae),\n",
    "        'rmse': float(test_rmse)\n",
    "    },\n",
    "    'train_metrics': {\n",
    "        'r2': float(train_r2),\n",
    "        'mae': float(train_mae),\n",
    "        'rmse': float(train_rmse)\n",
    "    },\n",
    "    'best_params': random_search.best_params_,\n",
    "    'cv_best_score': float(-random_search.best_score_),\n",
    "    'comparison_vs_linear': {\n",
    "        'r2_improvement': float(r2_improvement),\n",
    "        'mae_improvement': float(mae_improvement)\n",
    "    },\n",
    "    'model_artifacts': {\n",
    "        'sklearn_model': str(model_pkl_path),\n",
    "        'onnx_model': str(onnx_path),\n",
    "        'preprocessor': str(preprocessor_path)\n",
    "    },\n",
    "    'figures_generated': [str(f.name) for f in figures_dir.glob(\"*.png\")]\n",
    "}\n",
    "\n",
    "# Save metadata as JSON\n",
    "import json\n",
    "metadata_path = reports_dir / \"experiment_metadata.json\"\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(experiment_metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úì Experiment metadata saved to: {metadata_path}\")\n",
    "print(f\"\\nüéâ Issue #6 Complete - Random Forest with ONNX + SHAP\")\n",
    "print(f\"üìÅ All outputs saved in: {reports_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbca22f",
   "metadata": {},
   "source": [
    "## 8. Statistical Analysis: Covariance, Variance, and Distribution Analysis\n",
    "\n",
    "**PhD Advisor Feedback Implementation**: Comprehensive statistical characterization of model performance and data distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157ba683",
   "metadata": {},
   "source": [
    "### 8.1 Residual Analysis: Mean, Variance, and Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1922d7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate residuals for both training and test sets\n",
    "residuals_train = y_train - y_pred_train\n",
    "residuals_test = y_test - y_pred_test\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"RESIDUAL ANALYSIS - TRAINING SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Central Tendency:\")\n",
    "print(f\"  - Mean: {residuals_train.mean():.6f} years\")\n",
    "print(f\"  - Median: {np.median(residuals_train):.6f} years\")\n",
    "print(f\"  - Mode region: [{np.percentile(residuals_train, 45):.2f}, {np.percentile(residuals_train, 55):.2f}] years\")\n",
    "\n",
    "print(f\"\\nüìä Spread (Variance Metrics):\")\n",
    "print(f\"  - Variance (œÉ¬≤): {residuals_train.var():.6f}\")\n",
    "print(f\"  - Standard Deviation (œÉ): {residuals_train.std():.6f} years\")\n",
    "print(f\"  - Mean Absolute Deviation: {np.abs(residuals_train - residuals_train.mean()).mean():.6f} years\")\n",
    "print(f\"  - Range: [{residuals_train.min():.4f}, {residuals_train.max():.4f}] years\")\n",
    "print(f\"  - Interquartile Range (IQR): {np.percentile(residuals_train, 75) - np.percentile(residuals_train, 25):.4f} years\")\n",
    "\n",
    "print(f\"\\nüìä Mean Squared Error Components:\")\n",
    "print(f\"  - MSE: {mean_squared_error(y_train, y_pred_train):.6f}\")\n",
    "print(f\"  - RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.6f} years\")\n",
    "print(f\"  - Variance of Residuals: {residuals_train.var():.6f}\")\n",
    "\n",
    "print(f\"\\nüìä Distribution Shape:\")\n",
    "print(f\"  - Skewness: {pd.Series(residuals_train).skew():.6f}\")\n",
    "print(f\"  - Kurtosis: {pd.Series(residuals_train).kurtosis():.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESIDUAL ANALYSIS - TEST SET\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Central Tendency:\")\n",
    "print(f\"  - Mean: {residuals_test.mean():.6f} years\")\n",
    "print(f\"  - Median: {np.median(residuals_test):.6f} years\")\n",
    "print(f\"  - Mode region: [{np.percentile(residuals_test, 45):.2f}, {np.percentile(residuals_test, 55):.2f}] years\")\n",
    "\n",
    "print(f\"\\nüìä Spread (Variance Metrics):\")\n",
    "print(f\"  - Variance (œÉ¬≤): {residuals_test.var():.6f}\")\n",
    "print(f\"  - Standard Deviation (œÉ): {residuals_test.std():.6f} years\")\n",
    "print(f\"  - Mean Absolute Deviation: {np.abs(residuals_test - residuals_test.mean()).mean():.6f} years\")\n",
    "print(f\"  - Range: [{residuals_test.min():.4f}, {residuals_test.max():.4f}] years\")\n",
    "print(f\"  - Interquartile Range (IQR): {np.percentile(residuals_test, 75) - np.percentile(residuals_test, 25):.4f} years\")\n",
    "\n",
    "print(f\"\\nüìä Mean Squared Error Components:\")\n",
    "print(f\"  - MSE: {mean_squared_error(y_test, y_pred_test):.6f}\")\n",
    "print(f\"  - RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.6f} years\")\n",
    "print(f\"  - Variance of Residuals: {residuals_test.var():.6f}\")\n",
    "\n",
    "print(f\"\\nüìä Distribution Shape:\")\n",
    "print(f\"  - Skewness: {pd.Series(residuals_test).skew():.6f}\")\n",
    "print(f\"  - Kurtosis: {pd.Series(residuals_test).kurtosis():.6f}\")\n",
    "\n",
    "# Homoscedasticity check\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HOMOSCEDASTICITY ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nVariance Ratio (Train/Test): {residuals_train.var() / residuals_test.var():.4f}\")\n",
    "print(f\"‚úì Ideal ratio is close to 1.0 (indicates consistent error variance)\")\n",
    "if abs(residuals_train.var() / residuals_test.var() - 1.0) < 0.2:\n",
    "    print(f\"‚úì PASS: Residual variance is consistent between train and test sets\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  WARNING: Significant difference in residual variance between sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize residual distributions with detailed statistics\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Training residuals histogram\n",
    "ax1 = axes[0, 0]\n",
    "n_train, bins_train, patches_train = ax1.hist(residuals_train, bins=50, color='steelblue', \n",
    "                                               alpha=0.7, edgecolor='black', density=True)\n",
    "ax1.axvline(residuals_train.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {residuals_train.mean():.3f}')\n",
    "ax1.axvline(np.median(residuals_train), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(residuals_train):.3f}')\n",
    "# Add normal distribution overlay\n",
    "from scipy.stats import norm\n",
    "mu_train, std_train = residuals_train.mean(), residuals_train.std()\n",
    "xmin_train, xmax_train = ax1.get_xlim()\n",
    "x_train = np.linspace(xmin_train, xmax_train, 100)\n",
    "p_train = norm.pdf(x_train, mu_train, std_train)\n",
    "ax1.plot(x_train, p_train, 'k--', linewidth=2, label='Normal dist.')\n",
    "ax1.set_xlabel('Residuals (years)', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('Density', fontsize=11, fontweight='bold')\n",
    "ax1.set_title(f'Training Set Residuals Distribution\\nœÉ¬≤={residuals_train.var():.4f}, œÉ={residuals_train.std():.4f}', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Test residuals histogram\n",
    "ax2 = axes[0, 1]\n",
    "n_test, bins_test, patches_test = ax2.hist(residuals_test, bins=50, color='darkgreen', \n",
    "                                            alpha=0.7, edgecolor='black', density=True)\n",
    "ax2.axvline(residuals_test.mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {residuals_test.mean():.3f}')\n",
    "ax2.axvline(np.median(residuals_test), color='orange', linestyle='--', linewidth=2, label=f'Median: {np.median(residuals_test):.3f}')\n",
    "# Add normal distribution overlay\n",
    "mu_test, std_test = residuals_test.mean(), residuals_test.std()\n",
    "xmin_test, xmax_test = ax2.get_xlim()\n",
    "x_test = np.linspace(xmin_test, xmax_test, 100)\n",
    "p_test = norm.pdf(x_test, mu_test, std_test)\n",
    "ax2.plot(x_test, p_test, 'k--', linewidth=2, label='Normal dist.')\n",
    "ax2.set_xlabel('Residuals (years)', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Density', fontsize=11, fontweight='bold')\n",
    "ax2.set_title(f'Test Set Residuals Distribution\\nœÉ¬≤={residuals_test.var():.4f}, œÉ={residuals_test.std():.4f}', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot for training set (normality test)\n",
    "ax3 = axes[1, 0]\n",
    "from scipy.stats import probplot\n",
    "probplot(residuals_train, dist=\"norm\", plot=ax3)\n",
    "ax3.set_title('Training Set: Q-Q Plot (Normality Test)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Q-Q plot for test set\n",
    "ax4 = axes[1, 1]\n",
    "probplot(residuals_test, dist=\"norm\", plot=ax4)\n",
    "ax4.set_title('Test Set: Q-Q Plot (Normality Test)', fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'residuals_distribution_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Figure saved: {figures_dir / 'residuals_distribution_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4406dc56",
   "metadata": {},
   "source": [
    "### 8.2 Covariance Analysis: Predictions vs Actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc831202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance matrix for predictions vs actuals\n",
    "print(\"=\"*70)\n",
    "print(\"COVARIANCE ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Training set covariance\n",
    "cov_matrix_train = np.cov(y_train, y_pred_train)\n",
    "print(f\"\\nüìä Training Set Covariance Matrix:\")\n",
    "print(f\"  - Var(Actual): {cov_matrix_train[0, 0]:.4f}\")\n",
    "print(f\"  - Var(Predicted): {cov_matrix_train[1, 1]:.4f}\")\n",
    "print(f\"  - Cov(Actual, Predicted): {cov_matrix_train[0, 1]:.4f}\")\n",
    "print(f\"  - Correlation coefficient: {cov_matrix_train[0, 1] / np.sqrt(cov_matrix_train[0, 0] * cov_matrix_train[1, 1]):.6f}\")\n",
    "\n",
    "# Test set covariance\n",
    "cov_matrix_test = np.cov(y_test, y_pred_test)\n",
    "print(f\"\\nüìä Test Set Covariance Matrix:\")\n",
    "print(f\"  - Var(Actual): {cov_matrix_test[0, 0]:.4f}\")\n",
    "print(f\"  - Var(Predicted): {cov_matrix_test[1, 1]:.4f}\")\n",
    "print(f\"  - Cov(Actual, Predicted): {cov_matrix_test[0, 1]:.4f}\")\n",
    "print(f\"  - Correlation coefficient: {cov_matrix_test[0, 1] / np.sqrt(cov_matrix_test[0, 0] * cov_matrix_test[1, 1]):.6f}\")\n",
    "\n",
    "# Pearson and Spearman correlations\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "pearson_train, p_pearson_train = pearsonr(y_train, y_pred_train)\n",
    "spearman_train, p_spearman_train = spearmanr(y_train, y_pred_train)\n",
    "\n",
    "pearson_test, p_pearson_test = pearsonr(y_test, y_pred_test)\n",
    "spearman_test, p_spearman_test = spearmanr(y_test, y_pred_test)\n",
    "\n",
    "print(f\"\\nüìä Correlation Analysis (Training):\")\n",
    "print(f\"  - Pearson correlation: {pearson_train:.6f} (p-value: {p_pearson_train:.2e})\")\n",
    "print(f\"  - Spearman correlation: {spearman_train:.6f} (p-value: {p_spearman_train:.2e})\")\n",
    "\n",
    "print(f\"\\nüìä Correlation Analysis (Test):\")\n",
    "print(f\"  - Pearson correlation: {pearson_test:.6f} (p-value: {p_pearson_test:.2e})\")\n",
    "print(f\"  - Spearman correlation: {spearman_test:.6f} (p-value: {p_spearman_test:.2e})\")\n",
    "\n",
    "# Interpretation\n",
    "print(f\"\\nüí° Interpretation:\")\n",
    "print(f\"  - High Pearson r indicates strong linear relationship\")\n",
    "print(f\"  - High Spearman œÅ indicates strong monotonic relationship\")\n",
    "print(f\"  - p-values < 0.05 indicate statistical significance\")\n",
    "if pearson_test > 0.95 and p_pearson_test < 0.001:\n",
    "    print(f\"  ‚úì Excellent correlation between predictions and actual values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ded59f",
   "metadata": {},
   "source": [
    "### 8.3 Feature Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b450c093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive feature statistics\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE DISTRIBUTION STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Calculate statistics for all features\n",
    "feature_stats = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Mean': X.mean(),\n",
    "    'Std': X.std(),\n",
    "    'Min': X.min(),\n",
    "    'Q25': X.quantile(0.25),\n",
    "    'Median': X.median(),\n",
    "    'Q75': X.quantile(0.75),\n",
    "    'Max': X.max(),\n",
    "    'Variance': X.var(),\n",
    "    'Skewness': X.skew(),\n",
    "    'Kurtosis': X.kurtosis()\n",
    "})\n",
    "\n",
    "# Sort by variance (descending)\n",
    "feature_stats_sorted = feature_stats.sort_values('Variance', ascending=False)\n",
    "\n",
    "print(f\"\\nüìä Top 10 Features by Variance:\")\n",
    "print(feature_stats_sorted[['Feature', 'Mean', 'Std', 'Variance', 'Min', 'Max']].head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nüìä Overall Feature Statistics Summary:\")\n",
    "print(f\"  - Mean of means: {feature_stats['Mean'].mean():.4f}\")\n",
    "print(f\"  - Mean of standard deviations: {feature_stats['Std'].mean():.4f}\")\n",
    "print(f\"  - Mean of variances: {feature_stats['Variance'].mean():.4f}\")\n",
    "print(f\"  - Total features: {len(feature_stats)}\")\n",
    "\n",
    "# Identify features with extreme skewness or kurtosis\n",
    "extreme_skew = feature_stats[np.abs(feature_stats['Skewness']) > 2]\n",
    "extreme_kurt = feature_stats[np.abs(feature_stats['Kurtosis']) > 5]\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Features with Extreme Skewness (|skew| > 2): {len(extreme_skew)}\")\n",
    "if len(extreme_skew) > 0:\n",
    "    print(extreme_skew[['Feature', 'Skewness']].to_string(index=False))\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  Features with Extreme Kurtosis (|kurt| > 5): {len(extreme_kurt)}\")\n",
    "if len(extreme_kurt) > 0:\n",
    "    print(extreme_kurt[['Feature', 'Kurtosis']].head(10).to_string(index=False))\n",
    "\n",
    "# Save detailed statistics to CSV\n",
    "stats_path = reports_dir / 'feature_statistics.csv'\n",
    "feature_stats_sorted.to_csv(stats_path, index=False)\n",
    "print(f\"\\n‚úì Full feature statistics saved to: {stats_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab4f78d",
   "metadata": {},
   "source": [
    "### 8.4 Feature Importance: Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importance from Random Forest\n",
    "feature_importance = best_rf.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': feature_importance\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FEATURE IMPORTANCE SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Importance Distribution:\")\n",
    "print(f\"  - Minimum: {feature_importance.min():.6f}\")\n",
    "print(f\"  - Maximum: {feature_importance.max():.6f}\")\n",
    "print(f\"  - Mean: {feature_importance.mean():.6f}\")\n",
    "print(f\"  - Median: {np.median(feature_importance):.6f}\")\n",
    "print(f\"  - Std Dev: {feature_importance.std():.6f}\")\n",
    "print(f\"  - Variance: {feature_importance.var():.6f}\")\n",
    "print(f\"  - Sum: {feature_importance.sum():.6f} (should be ‚âà1.0)\")\n",
    "\n",
    "print(f\"\\nüìä Percentiles:\")\n",
    "for p in [10, 25, 50, 75, 90, 95, 99]:\n",
    "    print(f\"  - {p}th percentile: {np.percentile(feature_importance, p):.6f}\")\n",
    "\n",
    "print(f\"\\nüìä Top 15 Most Important Features:\")\n",
    "top_features = feature_importance_df.head(15)\n",
    "for idx, row in top_features.iterrows():\n",
    "    print(f\"  {row['Feature']:30s} : {row['Importance']:.6f} ({100*row['Importance']:.2f}%)\")\n",
    "\n",
    "# Calculate cumulative importance\n",
    "feature_importance_df['Cumulative_Importance'] = feature_importance_df['Importance'].cumsum()\n",
    "n_features_80 = (feature_importance_df['Cumulative_Importance'] <= 0.80).sum() + 1\n",
    "n_features_90 = (feature_importance_df['Cumulative_Importance'] <= 0.90).sum() + 1\n",
    "n_features_95 = (feature_importance_df['Cumulative_Importance'] <= 0.95).sum() + 1\n",
    "\n",
    "print(f\"\\nüìä Cumulative Importance Analysis:\")\n",
    "print(f\"  - Features for 80% importance: {n_features_80} / {len(feature_names)} ({100*n_features_80/len(feature_names):.1f}%)\")\n",
    "print(f\"  - Features for 90% importance: {n_features_90} / {len(feature_names)} ({100*n_features_90/len(feature_names):.1f}%)\")\n",
    "print(f\"  - Features for 95% importance: {n_features_95} / {len(feature_names)} ({100*n_features_95/len(feature_names):.1f}%)\")\n",
    "\n",
    "# Identify dominant features\n",
    "dominant_features = feature_importance_df[feature_importance_df['Importance'] > 0.05]\n",
    "print(f\"\\nüìä Dominant Features (importance > 5%): {len(dominant_features)}\")\n",
    "for idx, row in dominant_features.iterrows():\n",
    "    print(f\"  {row['Feature']:30s} : {100*row['Importance']:.2f}%\")\n",
    "\n",
    "# Save feature importance\n",
    "importance_path = reports_dir / 'feature_importance_rf.csv'\n",
    "feature_importance_df.to_csv(importance_path, index=False)\n",
    "print(f\"\\n‚úì Feature importance saved to: {importance_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f7f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# Top 20 features bar plot\n",
    "ax1 = axes[0, 0]\n",
    "top_20 = feature_importance_df.head(20)\n",
    "ax1.barh(range(len(top_20)), top_20['Importance'], color='steelblue', edgecolor='black')\n",
    "ax1.set_yticks(range(len(top_20)))\n",
    "ax1.set_yticklabels(top_20['Feature'], fontsize=9)\n",
    "ax1.set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "ax1.set_title('Top 20 Most Important Features', fontsize=12, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Importance distribution histogram\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(feature_importance, bins=50, color='darkgreen', alpha=0.7, edgecolor='black')\n",
    "ax2.axvline(feature_importance.mean(), color='red', linestyle='--', linewidth=2, \n",
    "            label=f'Mean: {feature_importance.mean():.5f}')\n",
    "ax2.axvline(np.median(feature_importance), color='orange', linestyle='--', linewidth=2,\n",
    "            label=f'Median: {np.median(feature_importance):.5f}')\n",
    "ax2.set_xlabel('Feature Importance', fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel('Frequency', fontsize=11, fontweight='bold')\n",
    "ax2.set_title('Distribution of Feature Importance Values', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=9)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Cumulative importance plot\n",
    "ax3 = axes[1, 0]\n",
    "sorted_importance = np.sort(feature_importance)[::-1]\n",
    "cumsum_importance = np.cumsum(sorted_importance)\n",
    "ax3.plot(range(1, len(cumsum_importance) + 1), cumsum_importance, linewidth=2, color='purple')\n",
    "ax3.axhline(y=0.80, color='red', linestyle='--', linewidth=1.5, label='80% threshold')\n",
    "ax3.axhline(y=0.90, color='orange', linestyle='--', linewidth=1.5, label='90% threshold')\n",
    "ax3.axhline(y=0.95, color='green', linestyle='--', linewidth=1.5, label='95% threshold')\n",
    "ax3.axvline(x=n_features_80, color='red', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "ax3.axvline(x=n_features_90, color='orange', linestyle=':', linewidth=1.5, alpha=0.5)\n",
    "ax3.set_xlabel('Number of Features', fontsize=11, fontweight='bold')\n",
    "ax3.set_ylabel('Cumulative Importance', fontsize=11, fontweight='bold')\n",
    "ax3.set_title(f'Cumulative Feature Importance\\n{n_features_80} features = 80%, {n_features_90} features = 90%', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax3.legend(fontsize=9)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot of importance values\n",
    "ax4 = axes[1, 1]\n",
    "bp = ax4.boxplot([feature_importance], vert=False, patch_artist=True, widths=0.5,\n",
    "                  boxprops=dict(facecolor='lightblue', edgecolor='black', linewidth=1.5),\n",
    "                  medianprops=dict(color='red', linewidth=2),\n",
    "                  whiskerprops=dict(color='black', linewidth=1.5),\n",
    "                  capprops=dict(color='black', linewidth=1.5))\n",
    "ax4.set_xlabel('Feature Importance', fontsize=11, fontweight='bold')\n",
    "ax4.set_title(f'Feature Importance Distribution Summary\\nMin={feature_importance.min():.5f}, Max={feature_importance.max():.5f}', \n",
    "              fontsize=12, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'feature_importance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Figure saved: {figures_dir / 'feature_importance_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000a450",
   "metadata": {},
   "source": [
    "### 8.5 Comprehensive Statistical Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531e79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive statistical report\n",
    "statistical_summary = {\n",
    "    'Model': 'Random Forest',\n",
    "    'Experiment_Date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    \n",
    "    # Model Performance\n",
    "    'Train_R2': train_r2,\n",
    "    'Test_R2': test_r2,\n",
    "    'Train_RMSE': train_rmse,\n",
    "    'Test_RMSE': test_rmse,\n",
    "    'Train_MAE': train_mae,\n",
    "    'Test_MAE': test_mae,\n",
    "    'Overfitting_Gap': overfit_gap,\n",
    "    \n",
    "    # Residual Statistics - Training\n",
    "    'Train_Residual_Mean': residuals_train.mean(),\n",
    "    'Train_Residual_Std': residuals_train.std(),\n",
    "    'Train_Residual_Variance': residuals_train.var(),\n",
    "    'Train_Residual_Skewness': pd.Series(residuals_train).skew(),\n",
    "    'Train_Residual_Kurtosis': pd.Series(residuals_train).kurtosis(),\n",
    "    'Train_Residual_Min': residuals_train.min(),\n",
    "    'Train_Residual_Max': residuals_train.max(),\n",
    "    \n",
    "    # Residual Statistics - Test\n",
    "    'Test_Residual_Mean': residuals_test.mean(),\n",
    "    'Test_Residual_Std': residuals_test.std(),\n",
    "    'Test_Residual_Variance': residuals_test.var(),\n",
    "    'Test_Residual_Skewness': pd.Series(residuals_test).skew(),\n",
    "    'Test_Residual_Kurtosis': pd.Series(residuals_test).kurtosis(),\n",
    "    'Test_Residual_Min': residuals_test.min(),\n",
    "    'Test_Residual_Max': residuals_test.max(),\n",
    "    \n",
    "    # Covariance\n",
    "    'Train_Var_Actual': cov_matrix_train[0, 0],\n",
    "    'Train_Var_Predicted': cov_matrix_train[1, 1],\n",
    "    'Train_Covariance': cov_matrix_train[0, 1],\n",
    "    'Test_Var_Actual': cov_matrix_test[0, 0],\n",
    "    'Test_Var_Predicted': cov_matrix_test[1, 1],\n",
    "    'Test_Covariance': cov_matrix_test[0, 1],\n",
    "    \n",
    "    # Correlations\n",
    "    'Train_Pearson_r': pearson_train,\n",
    "    'Train_Pearson_p': p_pearson_train,\n",
    "    'Train_Spearman_rho': spearman_train,\n",
    "    'Train_Spearman_p': p_spearman_train,\n",
    "    'Test_Pearson_r': pearson_test,\n",
    "    'Test_Pearson_p': p_pearson_test,\n",
    "    'Test_Spearman_rho': spearman_test,\n",
    "    'Test_Spearman_p': p_spearman_test,\n",
    "    \n",
    "    # Feature Importance\n",
    "    'Feature_Importance_Min': feature_importance.min(),\n",
    "    'Feature_Importance_Max': feature_importance.max(),\n",
    "    'Feature_Importance_Mean': feature_importance.mean(),\n",
    "    'Feature_Importance_Median': np.median(feature_importance),\n",
    "    'Feature_Importance_Std': feature_importance.std(),\n",
    "    'Feature_Importance_Variance': feature_importance.var(),\n",
    "    'N_Features_80pct': n_features_80,\n",
    "    'N_Features_90pct': n_features_90,\n",
    "    'N_Features_95pct': n_features_95,\n",
    "    'N_Dominant_Features': len(dominant_features),\n",
    "    \n",
    "    # Model Configuration\n",
    "    'N_Estimators': best_rf.n_estimators,\n",
    "    'Max_Depth': best_rf.max_depth,\n",
    "    'Min_Samples_Split': best_rf.min_samples_split,\n",
    "    'Min_Samples_Leaf': best_rf.min_samples_leaf,\n",
    "    'Max_Features': best_rf.max_features if isinstance(best_rf.max_features, float) else str(best_rf.max_features),\n",
    "    \n",
    "    # Dataset Info\n",
    "    'N_Train_Samples': len(y_train),\n",
    "    'N_Test_Samples': len(y_test),\n",
    "    'N_Features': len(feature_names)\n",
    "}\n",
    "\n",
    "# Convert to DataFrame for nice display\n",
    "summary_df = pd.DataFrame([statistical_summary]).T\n",
    "summary_df.columns = ['Value']\n",
    "\n",
    "# Save to CSV\n",
    "summary_path = reports_dir / 'statistical_summary_rf.csv'\n",
    "summary_df.to_csv(summary_path)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPREHENSIVE STATISTICAL SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìä All key statistics have been calculated and saved.\")\n",
    "print(f\"‚úì Summary saved to: {summary_path}\")\n",
    "print(f\"\\nüìã Summary Preview (first 20 metrics):\")\n",
    "print(summary_df.head(20).to_string())\n",
    "\n",
    "print(f\"\\n\\nüí° PhD Professor Feedback Addressed:\")\n",
    "print(f\"  ‚úì Covariance matrices calculated for train and test sets\")\n",
    "print(f\"  ‚úì Residual variance and distribution analyzed\")\n",
    "print(f\"  ‚úì Mean squared error decomposed\")\n",
    "print(f\"  ‚úì Feature distributions summarized (min, max, mean, variance)\")\n",
    "print(f\"  ‚úì Feature importance statistics computed\")\n",
    "print(f\"  ‚úì Normality tests performed (Q-Q plots)\")\n",
    "print(f\"  ‚úì All metrics saved to CSV files for further analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98722da",
   "metadata": {},
   "source": [
    "## 9. ONNX Model Export for Production\n",
    "\n",
    "Convert the trained Random Forest to ONNX format for cross-platform deployment and efficient inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141a2f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export Random Forest to ONNX format\n",
    "print(\"üîÑ Converting Random Forest to ONNX format...\")\n",
    "\n",
    "# Define input type for ONNX conversion\n",
    "n_features = X_train_processed.shape[1]\n",
    "initial_type = [('float_input', FloatTensorType([None, n_features]))]\n",
    "\n",
    "# Convert model\n",
    "onnx_model = convert_sklearn(\n",
    "    best_rf, \n",
    "    initial_types=initial_type,\n",
    "    target_opset=12\n",
    ")\n",
    "\n",
    "# Save ONNX model\n",
    "onnx_path = models_dir / 'random_forest_model.onnx'\n",
    "with open(onnx_path, \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "\n",
    "print(f\"‚úì ONNX model saved to: {onnx_path}\")\n",
    "print(f\"‚úì Model size: {onnx_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "# Validate ONNX model by making predictions\n",
    "print(\"\\nüîÑ Validating ONNX model...\")\n",
    "sess = rt.InferenceSession(str(onnx_path))\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "\n",
    "# Make predictions with ONNX\n",
    "X_test_float = X_test_processed.astype(np.float32)\n",
    "onnx_pred = sess.run([label_name], {input_name: X_test_float})[0]\n",
    "\n",
    "# Compare ONNX predictions with sklearn predictions\n",
    "prediction_diff = np.abs(y_pred_test - onnx_pred.flatten())\n",
    "max_diff = prediction_diff.max()\n",
    "mean_diff = prediction_diff.mean()\n",
    "\n",
    "print(f\"\\nüìä ONNX Validation Results:\")\n",
    "print(f\"  - Max prediction difference: {max_diff:.6f} years\")\n",
    "print(f\"  - Mean prediction difference: {mean_diff:.6f} years\")\n",
    "print(f\"  - Predictions match sklearn: {np.allclose(y_pred_test, onnx_pred.flatten(), rtol=1e-5)}\")\n",
    "\n",
    "if max_diff < 1e-4:\n",
    "    print(f\"  ‚úì EXCELLENT: ONNX model predictions match sklearn perfectly\")\n",
    "elif max_diff < 1e-2:\n",
    "    print(f\"  ‚úì GOOD: ONNX model predictions are very close to sklearn\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  WARNING: Significant differences detected\")\n",
    "\n",
    "# Save sklearn model as well\n",
    "sklearn_model_path = models_dir / 'random_forest_model.pkl'\n",
    "joblib.dump(best_rf, sklearn_model_path)\n",
    "print(f\"\\n‚úì Scikit-learn model saved to: {sklearn_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc70104",
   "metadata": {},
   "source": [
    "## 10. SHAP Explanations for Model Interpretability\n",
    "\n",
    "Use SHAP TreeExplainer to generate feature importance and individual prediction explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb74184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SHAP TreeExplainer (optimized for tree-based models)\n",
    "print(\"üîÑ Initializing SHAP TreeExplainer...\")\n",
    "explainer = shap.TreeExplainer(best_rf)\n",
    "\n",
    "# Calculate SHAP values for test set (use a sample for speed if dataset is large)\n",
    "sample_size = min(500, len(X_test_processed))\n",
    "X_test_sample = X_test_processed[:sample_size]\n",
    "\n",
    "print(f\"üîÑ Calculating SHAP values for {sample_size} test samples...\")\n",
    "shap_values = explainer.shap_values(X_test_sample)\n",
    "\n",
    "print(f\"‚úì SHAP values calculated\")\n",
    "print(f\"‚úì SHAP values shape: {shap_values.shape}\")\n",
    "print(f\"‚úì Base value (expected value): {explainer.expected_value:.4f} years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1fca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Summary Plot (feature importance across all samples)\n",
    "print(\"üìä Generating SHAP summary plot...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, show=False, max_display=20)\n",
    "plt.title('SHAP Feature Importance Summary\\n(Top 20 Features)', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'shap_summary_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Figure saved: {figures_dir / 'shap_summary_plot.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7de642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP Bar Plot (mean absolute SHAP values)\n",
    "print(\"üìä Generating SHAP bar plot...\")\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_test_sample, feature_names=feature_names, \n",
    "                  plot_type=\"bar\", show=False, max_display=20)\n",
    "plt.title('SHAP Feature Importance (Mean |SHAP value|)\\nTop 20 Features', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.savefig(figures_dir / 'shap_bar_plot.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Figure saved: {figures_dir / 'shap_bar_plot.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eebc5a",
   "metadata": {},
   "source": [
    "## 11. MLflow Experiment Tracking\n",
    "\n",
    "Log all metrics, parameters, and artifacts to MLflow for comparison with the Linear Regression baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6c69b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set experiment name (same as Linear Regression for comparison)\n",
    "EXPERIMENT_NAME = \"aging_prediction_models\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "print(f\"üîÑ Logging to MLflow experiment: {EXPERIMENT_NAME}\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"random_forest_optimized\"):\n",
    "    \n",
    "    # Log hyperparameters\n",
    "    mlflow.log_params({\n",
    "        'model': 'RandomForestRegressor',\n",
    "        'n_estimators': best_rf.n_estimators,\n",
    "        'max_depth': best_rf.max_depth,\n",
    "        'min_samples_split': best_rf.min_samples_split,\n",
    "        'min_samples_leaf': best_rf.min_samples_leaf,\n",
    "        'max_features': best_rf.max_features if isinstance(best_rf.max_features, float) else str(best_rf.max_features),\n",
    "        'bootstrap': best_rf.bootstrap,\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'n_features': len(feature_names),\n",
    "        'test_size': TEST_SIZE,\n",
    "        'cv_folds': CV_FOLDS,\n",
    "        'hp_search_iterations': N_ITER\n",
    "    })\n",
    "    \n",
    "    # Log performance metrics\n",
    "    mlflow.log_metrics({\n",
    "        # Test set metrics\n",
    "        'test_r2': test_r2,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        \n",
    "        # Training set metrics\n",
    "        'train_r2': train_r2,\n",
    "        'train_rmse': train_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        \n",
    "        # Cross-validation metric\n",
    "        'cv_mae': -random_search.best_score_,\n",
    "        \n",
    "        # Overfitting indicator\n",
    "        'overfitting_gap': overfit_gap,\n",
    "        \n",
    "        # Residual statistics\n",
    "        'test_residual_mean': residuals_test.mean(),\n",
    "        'test_residual_std': residuals_test.std(),\n",
    "        'test_residual_variance': residuals_test.var(),\n",
    "        \n",
    "        # Correlations\n",
    "        'test_pearson_r': pearson_test,\n",
    "        'test_spearman_rho': spearman_test,\n",
    "        \n",
    "        # Feature importance\n",
    "        'feature_importance_max': feature_importance.max(),\n",
    "        'feature_importance_mean': feature_importance.mean(),\n",
    "        'n_features_80pct': n_features_80,\n",
    "        'n_features_90pct': n_features_90\n",
    "    })\n",
    "    \n",
    "    # Log models\n",
    "    mlflow.sklearn.log_model(best_rf, \"random_forest_model\")\n",
    "    \n",
    "    # Log artifacts\n",
    "    mlflow.log_artifact(str(sklearn_model_path))\n",
    "    mlflow.log_artifact(str(onnx_path))\n",
    "    mlflow.log_artifact(str(preprocessor_path))\n",
    "    mlflow.log_artifact(str(importance_path))\n",
    "    mlflow.log_artifact(str(summary_path))\n",
    "    \n",
    "    # Log figures\n",
    "    for fig_file in figures_dir.glob('*.png'):\n",
    "        mlflow.log_artifact(str(fig_file))\n",
    "    \n",
    "    # Log tags for easy filtering\n",
    "    mlflow.set_tags({\n",
    "        'model_type': 'ensemble',\n",
    "        'model_family': 'tree_based',\n",
    "        'optimization': 'randomized_search',\n",
    "        'onnx_compatible': 'true',\n",
    "        'shap_explanations': 'true',\n",
    "        'issue': '#6',\n",
    "        'experiment_date': datetime.now().strftime('%Y-%m-%d')\n",
    "    })\n",
    "    \n",
    "    print(\"‚úì Metrics logged to MLflow\")\n",
    "    print(\"‚úì Parameters logged to MLflow\")\n",
    "    print(\"‚úì Model artifacts logged to MLflow\")\n",
    "    print(\"‚úì Figures logged to MLflow\")\n",
    "    print(\"‚úì Tags set for easy filtering\")\n",
    "    \n",
    "print(f\"\\nüéØ MLflow Run Complete!\")\n",
    "print(f\"View results: mlflow ui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dab9af",
   "metadata": {},
   "source": [
    "## 12. Final Summary and Conclusions\n",
    "\n",
    "This section provides a comprehensive summary of the Random Forest experiment, comparison with the Linear Regression baseline, and key insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada87530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# COMPREHENSIVE EXPERIMENT SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"RANDOM FOREST AGING PREDICTION MODEL - FINAL SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Model Information\n",
    "print(\"üìä MODEL ARCHITECTURE\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Algorithm:           RandomForestRegressor (Ensemble Method)\")\n",
    "print(f\"  Optimization:        RandomizedSearchCV ({N_ITER} iterations, {CV_FOLDS}-fold CV)\")\n",
    "print(f\"  Best Estimators:     {best_rf.n_estimators} trees\")\n",
    "print(f\"  Max Depth:           {best_rf.max_depth}\")\n",
    "print(f\"  Min Samples Split:   {best_rf.min_samples_split}\")\n",
    "print(f\"  Min Samples Leaf:    {best_rf.min_samples_leaf}\")\n",
    "print(f\"  Max Features:        {best_rf.max_features}\")\n",
    "print(f\"  Bootstrap:           {best_rf.bootstrap}\")\n",
    "print()\n",
    "\n",
    "# Dataset Information\n",
    "print(\"üìÅ DATASET CHARACTERISTICS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Total Samples:       {len(df)} (train: {len(X_train)}, test: {len(X_test)})\")\n",
    "print(f\"  Total Features:      {len(feature_names)}\")\n",
    "print(f\"  Target Variable:     Biological Age (years)\")\n",
    "print(f\"  Age Range:           {y.min():.1f} - {y.max():.1f} years\")\n",
    "print(f\"  Age Mean ¬± SD:       {y.mean():.1f} ¬± {y.std():.1f} years\")\n",
    "print(f\"  Data Quality:        {df.isnull().sum().sum()} missing values\")\n",
    "print()\n",
    "\n",
    "# Performance Metrics\n",
    "print(\"üéØ MODEL PERFORMANCE\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Test R¬≤ Score:       {test_r2:.4f}  {'‚≠ê' if test_r2 > 0.95 else '‚úì' if test_r2 > 0.90 else '‚ö†Ô∏è'}\")\n",
    "print(f\"  Test RMSE:           {test_rmse:.2f} years\")\n",
    "print(f\"  Test MAE:            {test_mae:.2f} years\")\n",
    "print()\n",
    "print(f\"  Train R¬≤ Score:      {train_r2:.4f}\")\n",
    "print(f\"  Train RMSE:          {train_rmse:.2f} years\")\n",
    "print(f\"  Train MAE:           {train_mae:.2f} years\")\n",
    "print()\n",
    "print(f\"  CV MAE:              {-random_search.best_score_:.2f} years\")\n",
    "print(f\"  Overfitting Gap:     {overfit_gap:.4f}  {'‚úì Good' if overfit_gap < 0.03 else '‚ö†Ô∏è Moderate' if overfit_gap < 0.05 else '‚ùå High'}\")\n",
    "print()\n",
    "\n",
    "# Statistical Analysis\n",
    "print(\"üìà STATISTICAL ANALYSIS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Residual Mean:       {residuals_test.mean():.4f} years (ideal: ~0)\")\n",
    "print(f\"  Residual Std:        {residuals_test.std():.2f} years\")\n",
    "print(f\"  Residual Variance:   {residuals_test.var():.2f}\")\n",
    "print(f\"  Pearson r:           {pearson_test:.4f} (p={pearson_p_test:.2e})\")\n",
    "print(f\"  Spearman œÅ:          {spearman_test:.4f} (p={spearman_p_test:.2e})\")\n",
    "print()\n",
    "\n",
    "# Feature Importance\n",
    "print(\"üîç FEATURE IMPORTANCE INSIGHTS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  Top Feature:         {top_features.iloc[0]['Feature']} ({top_features.iloc[0]['Importance']:.4f})\")\n",
    "print(f\"  Top 5 Features:      {', '.join(top_features.head(5)['Feature'].values)}\")\n",
    "print(f\"  Features (80%):      {n_features_80} features explain 80% of variance\")\n",
    "print(f\"  Features (90%):      {n_features_90} features explain 90% of variance\")\n",
    "print(f\"  Importance Range:    {feature_importance.min():.6f} - {feature_importance.max():.6f}\")\n",
    "print()\n",
    "\n",
    "# Model Artifacts\n",
    "print(\"üíæ EXPORTED ARTIFACTS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"  ‚úì Random Forest Model:     {sklearn_model_path.name}\")\n",
    "print(f\"  ‚úì ONNX Model:              {onnx_path.name}\")\n",
    "print(f\"  ‚úì Preprocessor:            {preprocessor_path.name}\")\n",
    "print(f\"  ‚úì Feature Importance:      {importance_path.name}\")\n",
    "print(f\"  ‚úì Statistical Summary:     {summary_path.name}\")\n",
    "print(f\"  ‚úì Visualizations:          {len(list(figures_dir.glob('*.png')))} figures\")\n",
    "print()\n",
    "\n",
    "# Baseline Comparison\n",
    "print(\"üìä COMPARISON WITH LINEAR REGRESSION BASELINE (Issue #21)\")\n",
    "print(\"-\" * 80)\n",
    "baseline_r2 = 0.9691\n",
    "baseline_mae = 2.05\n",
    "print(f\"  Linear Regression R¬≤:      {baseline_r2:.4f}\")\n",
    "print(f\"  Random Forest R¬≤:          {test_r2:.4f}\")\n",
    "print(f\"  R¬≤ Improvement:            {(test_r2 - baseline_r2):.4f} ({(test_r2 - baseline_r2)/baseline_r2*100:+.2f}%)\")\n",
    "print()\n",
    "print(f\"  Linear Regression MAE:     {baseline_mae:.2f} years\")\n",
    "print(f\"  Random Forest MAE:         {test_mae:.2f} years\")\n",
    "print(f\"  MAE Improvement:           {(baseline_mae - test_mae):.2f} years ({(baseline_mae - test_mae)/baseline_mae*100:+.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Interpretation\n",
    "print(\"üéì KEY FINDINGS\")\n",
    "print(\"-\" * 80)\n",
    "if test_r2 > baseline_r2:\n",
    "    print(\"  ‚úì Random Forest outperforms Linear Regression baseline\")\n",
    "    print(\"  ‚úì Non-linear relationships in aging biomarkers captured effectively\")\n",
    "    print(\"  ‚úì Ensemble approach reduces prediction variance\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ Random Forest performs comparably to Linear Regression\")\n",
    "    print(\"  ‚Ä¢ Data may have predominantly linear relationships\")\n",
    "    print(\"  ‚Ä¢ Simpler model (Linear) might be preferred for interpretability\")\n",
    "\n",
    "if overfit_gap < 0.03:\n",
    "    print(\"  ‚úì Model generalizes well (minimal overfitting)\")\n",
    "elif overfit_gap < 0.05:\n",
    "    print(\"  ‚ö†Ô∏è Moderate overfitting detected (consider regularization)\")\n",
    "else:\n",
    "    print(\"  ‚ùå Significant overfitting (model too complex for data)\")\n",
    "\n",
    "if abs(residuals_test.mean()) < 0.5:\n",
    "    print(\"  ‚úì Predictions are unbiased (residual mean ‚âà 0)\")\n",
    "else:\n",
    "    print(\"  ‚ö†Ô∏è Systematic bias detected in predictions\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Limitations and Future Work\n",
    "print(\"‚ö†Ô∏è LIMITATIONS\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  ‚Ä¢ Model trained on simulated/synthetic data\")\n",
    "print(\"  ‚Ä¢ Feature interactions not explicitly modeled\")\n",
    "print(\"  ‚Ä¢ Cross-sectional data (longitudinal validation needed)\")\n",
    "print(\"  ‚Ä¢ Biological interpretability of ensemble models challenging\")\n",
    "print()\n",
    "\n",
    "print(\"üöÄ NEXT STEPS (From ROADMAP.md)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"  1. Test model on independent validation cohort\")\n",
    "print(\"  2. Implement XGBoost (Issue #7) for gradient boosting comparison\")\n",
    "print(\"  3. Add biological pathway analysis with SHAP interactions\")\n",
    "print(\"  4. Deploy best model to production API\")\n",
    "print(\"  5. Conduct longitudinal validation study\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úì Experiment completed successfully on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"‚úì All results logged to MLflow experiment: {EXPERIMENT_NAME}\")\n",
    "print(f\"‚úì Artifacts saved to: {models_dir}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
